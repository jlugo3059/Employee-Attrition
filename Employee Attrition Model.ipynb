{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbe80f9",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50353520",
   "metadata": {},
   "source": [
    "The purpose of this project is to create both a neural network and a logistic regression of an attrition dataset. After creating both models we will then compare both of them to see which one is better.  The data for this particular project will be derived from Kaggle.  \n",
    "https://www.kaggle.com/datasets/rishikeshkonapure/hr-analytics-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27cf7f",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc8b58",
   "metadata": {},
   "source": [
    "Employee attrition is a common problem many companies today face. While the data for this particular project cannot represent scenarios or variables that every company can face , it does provide a general idea of what may be potential problems. With that being said our goal for this project will be to use the datasets independent variables to help predict attrition. Note, that in this project the target variable will be **‘attrition’**  which is a binary variable of ‘yes’ or ‘no’. Yes, in the fact that the employee quit/ left their job and no in the fact that they did not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c4b9f",
   "metadata": {},
   "source": [
    "# Importing the data and packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72881de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947b50a",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139a373",
   "metadata": {},
   "source": [
    "By running the code data.info() we can both check the size of our data and check if null values exist. The results from running this code tell us data we have 1,470 observations and 35 variables. Note that we have both numerical and categorical variables. Similarly, note that our dataset does not have existing null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f148b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 35 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   Age                       1470 non-null   int64 \n",
      " 1   Attrition                 1470 non-null   object\n",
      " 2   BusinessTravel            1470 non-null   object\n",
      " 3   DailyRate                 1470 non-null   int64 \n",
      " 4   Department                1470 non-null   object\n",
      " 5   DistanceFromHome          1470 non-null   int64 \n",
      " 6   Education                 1470 non-null   int64 \n",
      " 7   EducationField            1470 non-null   object\n",
      " 8   EmployeeCount             1470 non-null   int64 \n",
      " 9   EmployeeNumber            1470 non-null   int64 \n",
      " 10  EnvironmentSatisfaction   1470 non-null   int64 \n",
      " 11  Gender                    1470 non-null   object\n",
      " 12  HourlyRate                1470 non-null   int64 \n",
      " 13  JobInvolvement            1470 non-null   int64 \n",
      " 14  JobLevel                  1470 non-null   int64 \n",
      " 15  JobRole                   1470 non-null   object\n",
      " 16  JobSatisfaction           1470 non-null   int64 \n",
      " 17  MaritalStatus             1470 non-null   object\n",
      " 18  MonthlyIncome             1470 non-null   int64 \n",
      " 19  MonthlyRate               1470 non-null   int64 \n",
      " 20  NumCompaniesWorked        1470 non-null   int64 \n",
      " 21  Over18                    1470 non-null   object\n",
      " 22  OverTime                  1470 non-null   object\n",
      " 23  PercentSalaryHike         1470 non-null   int64 \n",
      " 24  PerformanceRating         1470 non-null   int64 \n",
      " 25  RelationshipSatisfaction  1470 non-null   int64 \n",
      " 26  StandardHours             1470 non-null   int64 \n",
      " 27  StockOptionLevel          1470 non-null   int64 \n",
      " 28  TotalWorkingYears         1470 non-null   int64 \n",
      " 29  TrainingTimesLastYear     1470 non-null   int64 \n",
      " 30  WorkLifeBalance           1470 non-null   int64 \n",
      " 31  YearsAtCompany            1470 non-null   int64 \n",
      " 32  YearsInCurrentRole        1470 non-null   int64 \n",
      " 33  YearsSinceLastPromotion   1470 non-null   int64 \n",
      " 34  YearsWithCurrManager      1470 non-null   int64 \n",
      "dtypes: int64(26), object(9)\n",
      "memory usage: 402.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cdffae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
       "1   49        No  Travel_Frequently        279  Research & Development   \n",
       "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
       "3   33        No  Travel_Frequently       1392  Research & Development   \n",
       "4   27        No      Travel_Rarely        591  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "\n",
       "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
       "0  ...                         1            80                 0   \n",
       "1  ...                         4            80                 1   \n",
       "2  ...                         2            80                 0   \n",
       "3  ...                         3            80                 0   \n",
       "4  ...                         4            80                 1   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0               1               6   \n",
       "1                 10                      3               3              10   \n",
       "2                  7                      3               3               0   \n",
       "3                  8                      3               3               8   \n",
       "4                  6                      3               3               2   \n",
       "\n",
       "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                  4                        0                     5  \n",
       "1                  7                        1                     7  \n",
       "2                  0                        0                     0  \n",
       "3                  7                        3                     0  \n",
       "4                  2                        2                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets look at the first 5 observations\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e6710",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c34c1d",
   "metadata": {},
   "source": [
    "Based on data exploration we have categorical variables in our dataset. Therefor we will check the unique number of categories in each variable before creating dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122706c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition: 2 unique categories\n",
      "BusinessTravel: 3 unique categories\n",
      "Department: 3 unique categories\n",
      "EducationField: 6 unique categories\n",
      "Gender: 2 unique categories\n",
      "JobRole: 9 unique categories\n",
      "MaritalStatus: 3 unique categories\n",
      "Over18: 1 unique categories\n",
      "OverTime: 2 unique categories\n"
     ]
    }
   ],
   "source": [
    "#lets check the number of unique categories in our categorical variables\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':  # if the column is categorical\n",
    "        num_unique_categories = data[column].nunique()\n",
    "        print(f'{column}: {num_unique_categories} unique categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d502f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd85e52",
   "metadata": {},
   "source": [
    "The number of unique categories for each categorical variable is low , therefor we can proceed into making dummy variables. Note that we will first start by hardcoding our target variable **attrition**. The reason behind this is because when we run the code *data = pd.get_dummies(data, drop_first=True)* pandas has a hard time interpreting the 'attrition' variable and drops it from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0d09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Attrition'] = data['Attrition'].map({'No': 0, 'Yes': 1})\n",
    "data = pd.get_dummies(data, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06cb25",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e13a0",
   "metadata": {},
   "source": [
    "Let us now double check the dummy variables that were created. The data type for categorical variables is now uint8 instead of object. Similarly, note that hardcoding attrition changes its data type to int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741d8d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                  int64\n",
       "Attrition                            int64\n",
       "DailyRate                            int64\n",
       "DistanceFromHome                     int64\n",
       "Education                            int64\n",
       "EmployeeCount                        int64\n",
       "EmployeeNumber                       int64\n",
       "EnvironmentSatisfaction              int64\n",
       "HourlyRate                           int64\n",
       "JobInvolvement                       int64\n",
       "JobLevel                             int64\n",
       "JobSatisfaction                      int64\n",
       "MonthlyIncome                        int64\n",
       "MonthlyRate                          int64\n",
       "NumCompaniesWorked                   int64\n",
       "PercentSalaryHike                    int64\n",
       "PerformanceRating                    int64\n",
       "RelationshipSatisfaction             int64\n",
       "StandardHours                        int64\n",
       "StockOptionLevel                     int64\n",
       "TotalWorkingYears                    int64\n",
       "TrainingTimesLastYear                int64\n",
       "WorkLifeBalance                      int64\n",
       "YearsAtCompany                       int64\n",
       "YearsInCurrentRole                   int64\n",
       "YearsSinceLastPromotion              int64\n",
       "YearsWithCurrManager                 int64\n",
       "BusinessTravel_Travel_Frequently     uint8\n",
       "BusinessTravel_Travel_Rarely         uint8\n",
       "Department_Research & Development    uint8\n",
       "Department_Sales                     uint8\n",
       "EducationField_Life Sciences         uint8\n",
       "EducationField_Marketing             uint8\n",
       "EducationField_Medical               uint8\n",
       "EducationField_Other                 uint8\n",
       "EducationField_Technical Degree      uint8\n",
       "Gender_Male                          uint8\n",
       "JobRole_Human Resources              uint8\n",
       "JobRole_Laboratory Technician        uint8\n",
       "JobRole_Manager                      uint8\n",
       "JobRole_Manufacturing Director       uint8\n",
       "JobRole_Research Director            uint8\n",
       "JobRole_Research Scientist           uint8\n",
       "JobRole_Sales Executive              uint8\n",
       "JobRole_Sales Representative         uint8\n",
       "MaritalStatus_Married                uint8\n",
       "MaritalStatus_Single                 uint8\n",
       "OverTime_Yes                         uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks the datatypes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00095b",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197272a",
   "metadata": {},
   "source": [
    "Before we can move on to feature scaling we must first convert **Attrition** to a non-numeric data type. If we don’t, attrition will be featured scaled along all the other numeric variables in the dataset. Therefor its best to change Attrition to the same data type as all the other dummy variables (uint8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27faf56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data type of Attrition to uint8\n",
    "data['Attrition'] = data['Attrition'].astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458a31c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40930575",
   "metadata": {},
   "source": [
    "# Feature Scaling : Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2779ca",
   "metadata": {},
   "source": [
    "If you recall in data exploration, when we used the code '*data.head()*' we had variables like Daily rain and Employee count. Daily rain had values ranging up to a thousand while employee count had values of one. This tells us that our dataset needs to be featured scaled to put variables into a similar scale. For this example, I will use standardization which is a feature scaling method that will scale the predicter value data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Note that only the numeric variables should be scaled. Therefor, were going to isolate the numeric variables and scaled them in a new function called **data_scaled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec7426f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select only the numeric columns\n",
    "numeric_columns = data.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "\n",
    "# Fit the scaler and transform the data\n",
    "data_scaled = scaler.fit_transform(numeric_columns)\n",
    "\n",
    "# Convert back to a dataframe\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=numeric_columns.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f41410",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845019c",
   "metadata": {},
   "source": [
    "Simirlaly were going to also isolate the dummy variables we created and stored them in a new function called **dummy_columns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74dc87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dummy variables from the original DataFrame\n",
    "dummy_columns = data.select_dtypes(include=['uint8'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fb769",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff6faf",
   "metadata": {},
   "source": [
    "Now that the numeric variables are scaled in **data_scaled** , we need to merger them together with **dummy_columns** to get an updated dataset that has both featured scaled numeric numbers and dummy variables. Note, that it is important to reset the indices before concatenating, because if we don’t pandas can create errors in the merger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80303c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the indices of both DataFrames\n",
    "data_scaled = data_scaled.reset_index(drop=True)\n",
    "dummy_columns = dummy_columns.reset_index(drop=True)\n",
    "\n",
    "# Combine the scaled DataFrame and the dummy variables\n",
    "data_scaled = pd.concat([data_scaled, dummy_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd758c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Laboratory Technician</th>\n",
       "      <th>JobRole_Manager</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.446350</td>\n",
       "      <td>0.742527</td>\n",
       "      <td>-1.010909</td>\n",
       "      <td>-0.891688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.701283</td>\n",
       "      <td>-0.660531</td>\n",
       "      <td>1.383138</td>\n",
       "      <td>0.379672</td>\n",
       "      <td>-0.057788</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.322365</td>\n",
       "      <td>-1.297775</td>\n",
       "      <td>-0.147150</td>\n",
       "      <td>-1.868426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.699621</td>\n",
       "      <td>0.254625</td>\n",
       "      <td>-0.240677</td>\n",
       "      <td>-1.026167</td>\n",
       "      <td>-0.057788</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008343</td>\n",
       "      <td>1.414363</td>\n",
       "      <td>-0.887515</td>\n",
       "      <td>-0.891688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.696298</td>\n",
       "      <td>1.169781</td>\n",
       "      <td>1.284725</td>\n",
       "      <td>-1.026167</td>\n",
       "      <td>-0.961486</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.429664</td>\n",
       "      <td>1.461466</td>\n",
       "      <td>-0.764121</td>\n",
       "      <td>1.061787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.694636</td>\n",
       "      <td>1.169781</td>\n",
       "      <td>-0.486709</td>\n",
       "      <td>0.379672</td>\n",
       "      <td>-0.961486</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.086676</td>\n",
       "      <td>-0.524295</td>\n",
       "      <td>-0.887515</td>\n",
       "      <td>-1.868426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.691313</td>\n",
       "      <td>-1.575686</td>\n",
       "      <td>-1.274014</td>\n",
       "      <td>0.379672</td>\n",
       "      <td>-0.961486</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>-0.101159</td>\n",
       "      <td>0.202082</td>\n",
       "      <td>1.703764</td>\n",
       "      <td>-0.891688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.721670</td>\n",
       "      <td>0.254625</td>\n",
       "      <td>-1.224807</td>\n",
       "      <td>1.785511</td>\n",
       "      <td>-0.057788</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>0.227347</td>\n",
       "      <td>-0.469754</td>\n",
       "      <td>-0.393938</td>\n",
       "      <td>-1.868426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.723332</td>\n",
       "      <td>1.169781</td>\n",
       "      <td>-1.175601</td>\n",
       "      <td>-1.026167</td>\n",
       "      <td>0.845911</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>-1.086676</td>\n",
       "      <td>-1.605183</td>\n",
       "      <td>-0.640727</td>\n",
       "      <td>0.085049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.726655</td>\n",
       "      <td>-0.660531</td>\n",
       "      <td>1.038693</td>\n",
       "      <td>1.785511</td>\n",
       "      <td>-0.057788</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>1.322365</td>\n",
       "      <td>0.546677</td>\n",
       "      <td>-0.887515</td>\n",
       "      <td>0.085049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.728317</td>\n",
       "      <td>1.169781</td>\n",
       "      <td>-0.142264</td>\n",
       "      <td>-1.026167</td>\n",
       "      <td>-0.057788</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>-0.320163</td>\n",
       "      <td>-0.432568</td>\n",
       "      <td>-0.147150</td>\n",
       "      <td>0.085049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.733302</td>\n",
       "      <td>-0.660531</td>\n",
       "      <td>0.792660</td>\n",
       "      <td>1.785511</td>\n",
       "      <td>-0.057788</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1470 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age  DailyRate  DistanceFromHome  Education  EmployeeCount  \\\n",
       "0     0.446350   0.742527         -1.010909  -0.891688            0.0   \n",
       "1     1.322365  -1.297775         -0.147150  -1.868426            0.0   \n",
       "2     0.008343   1.414363         -0.887515  -0.891688            0.0   \n",
       "3    -0.429664   1.461466         -0.764121   1.061787            0.0   \n",
       "4    -1.086676  -0.524295         -0.887515  -1.868426            0.0   \n",
       "...        ...        ...               ...        ...            ...   \n",
       "1465 -0.101159   0.202082          1.703764  -0.891688            0.0   \n",
       "1466  0.227347  -0.469754         -0.393938  -1.868426            0.0   \n",
       "1467 -1.086676  -1.605183         -0.640727   0.085049            0.0   \n",
       "1468  1.322365   0.546677         -0.887515   0.085049            0.0   \n",
       "1469 -0.320163  -0.432568         -0.147150   0.085049            0.0   \n",
       "\n",
       "      EmployeeNumber  EnvironmentSatisfaction  HourlyRate  JobInvolvement  \\\n",
       "0          -1.701283                -0.660531    1.383138        0.379672   \n",
       "1          -1.699621                 0.254625   -0.240677       -1.026167   \n",
       "2          -1.696298                 1.169781    1.284725       -1.026167   \n",
       "3          -1.694636                 1.169781   -0.486709        0.379672   \n",
       "4          -1.691313                -1.575686   -1.274014        0.379672   \n",
       "...              ...                      ...         ...             ...   \n",
       "1465        1.721670                 0.254625   -1.224807        1.785511   \n",
       "1466        1.723332                 1.169781   -1.175601       -1.026167   \n",
       "1467        1.726655                -0.660531    1.038693        1.785511   \n",
       "1468        1.728317                 1.169781   -0.142264       -1.026167   \n",
       "1469        1.733302                -0.660531    0.792660        1.785511   \n",
       "\n",
       "      JobLevel  ...  JobRole_Laboratory Technician  JobRole_Manager  \\\n",
       "0    -0.057788  ...                              0                0   \n",
       "1    -0.057788  ...                              0                0   \n",
       "2    -0.961486  ...                              1                0   \n",
       "3    -0.961486  ...                              0                0   \n",
       "4    -0.961486  ...                              1                0   \n",
       "...        ...  ...                            ...              ...   \n",
       "1465 -0.057788  ...                              1                0   \n",
       "1466  0.845911  ...                              0                0   \n",
       "1467 -0.057788  ...                              0                0   \n",
       "1468 -0.057788  ...                              0                0   \n",
       "1469 -0.057788  ...                              1                0   \n",
       "\n",
       "      JobRole_Manufacturing Director  JobRole_Research Director  \\\n",
       "0                                  0                          0   \n",
       "1                                  0                          0   \n",
       "2                                  0                          0   \n",
       "3                                  0                          0   \n",
       "4                                  0                          0   \n",
       "...                              ...                        ...   \n",
       "1465                               0                          0   \n",
       "1466                               0                          0   \n",
       "1467                               1                          0   \n",
       "1468                               0                          0   \n",
       "1469                               0                          0   \n",
       "\n",
       "      JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
       "0                              0                        1   \n",
       "1                              1                        0   \n",
       "2                              0                        0   \n",
       "3                              1                        0   \n",
       "4                              0                        0   \n",
       "...                          ...                      ...   \n",
       "1465                           0                        0   \n",
       "1466                           0                        0   \n",
       "1467                           0                        0   \n",
       "1468                           0                        1   \n",
       "1469                           0                        0   \n",
       "\n",
       "      JobRole_Sales Representative  MaritalStatus_Married  \\\n",
       "0                                0                      0   \n",
       "1                                0                      1   \n",
       "2                                0                      0   \n",
       "3                                0                      1   \n",
       "4                                0                      1   \n",
       "...                            ...                    ...   \n",
       "1465                             0                      1   \n",
       "1466                             0                      1   \n",
       "1467                             0                      1   \n",
       "1468                             0                      1   \n",
       "1469                             0                      1   \n",
       "\n",
       "      MaritalStatus_Single  OverTime_Yes  \n",
       "0                        1             1  \n",
       "1                        0             0  \n",
       "2                        1             1  \n",
       "3                        0             1  \n",
       "4                        0             0  \n",
       "...                    ...           ...  \n",
       "1465                     0             0  \n",
       "1466                     0             0  \n",
       "1467                     0             1  \n",
       "1468                     0             0  \n",
       "1469                     0             0  \n",
       "\n",
       "[1470 rows x 48 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a889c",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207321e5",
   "metadata": {},
   "source": [
    "Now that our data is scaled and has dummy variables lets split the data into training and testing sets. I first start by creating a new variable called **features** which eliminates the target variable attrition from the rest of the predicter variables. Then I create a variable called **target** which will only include data from the attrition variable. By creating two variables called **features** and **target** I will able to isolate the predicter values from the attrition variable, forming two distinct training(**X_train** and **y_train**) and testing sets (**X_test** and **y_test**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e06f37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits our data into two variables features(predicter variables) and  target(attrition)\n",
    "features = data_scaled.drop('Attrition', axis=1)\n",
    "target = data_scaled['Attrition']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b040582",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd70634",
   "metadata": {},
   "source": [
    "Now we can start building the neural network . For this example I choose to go with three layers and start with 32 units. The ReLU function is preferred in neural networks because it is computationally efficient, easy to implement, and has been shown to work well in similar neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9360fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeeb21a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f689264",
   "metadata": {},
   "source": [
    "Compiling the neural network is next , for this I used 'adam' a very popular optimizer that deals really well with noisy data. Similarly, BinerayCrossentropy is commonly used for binary classification problems like ours. Logits=True argument means that the model's final layer has not been passed through an activation function, and the model is outputting the direct logits of the last layer. This is a common setup when using binary cross-entropy, as it makes the model more numerically stable. Lastly, Accuracy provides a simple measure of the proportion of correctly predicted instances, we will use this measure to compare this models accuracy againts another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "257907f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d22b9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b430ef",
   "metadata": {},
   "source": [
    "Now we can start training our model with our training data . For this we used 10 epochs and a batch size of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24722a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "118/118 [==============================] - 0s 592us/step - loss: 0.5589 - accuracy: 0.7993\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - 0s 584us/step - loss: 0.4214 - accuracy: 0.8316\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - 0s 588us/step - loss: 0.3796 - accuracy: 0.8316\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - 0s 581us/step - loss: 0.3488 - accuracy: 0.8393\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - 0s 583us/step - loss: 0.3215 - accuracy: 0.8631\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - 0s 579us/step - loss: 0.3005 - accuracy: 0.8724\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - 0s 581us/step - loss: 0.2856 - accuracy: 0.8827\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - 0s 570us/step - loss: 0.2713 - accuracy: 0.8912\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - 0s 571us/step - loss: 0.2564 - accuracy: 0.8903\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - 0s 578us/step - loss: 0.2449 - accuracy: 0.8903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c88d58e550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ca8e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03edb3bd",
   "metadata": {},
   "source": [
    "Afterwards, we can call on the Loss function using the same metrics we used on the compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9e310d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778a779",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3746f2d",
   "metadata": {},
   "source": [
    "This is where everything gets put together, in summary this code is training the model on the X_train data and y_train labels for 10 epochs, evaluating it against the X_test data and y_test labels after each epoch, and storing the history of this process in the function **history** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66d6263e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 [==============================] - 1s 4ms/step - loss: 0.2295 - accuracy: 0.9014 - val_loss: 0.3137 - val_accuracy: 0.8844\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2208 - accuracy: 0.9039 - val_loss: 0.3142 - val_accuracy: 0.8844\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 0.9073 - val_loss: 0.3156 - val_accuracy: 0.8844\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9082 - val_loss: 0.3204 - val_accuracy: 0.8912\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9150 - val_loss: 0.3198 - val_accuracy: 0.8810\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9133 - val_loss: 0.3243 - val_accuracy: 0.8878\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9175 - val_loss: 0.3267 - val_accuracy: 0.8844\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1875 - accuracy: 0.9218 - val_loss: 0.3274 - val_accuracy: 0.8810\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9218 - val_loss: 0.3284 - val_accuracy: 0.8878\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9252 - val_loss: 0.3314 - val_accuracy: 0.8844\n"
     ]
    }
   ],
   "source": [
    "# where the training of the neural network actually happens\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adeb77",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04579e",
   "metadata": {},
   "source": [
    "The neural network is now complete, lets test the accuracy of the model's predictions by comparing them to the true labels of the test set. \n",
    "\n",
    "The results from running the code below indicate that the Neural Network accuracy score was 0.884 which meant that the model correctly predicted the attrition variable ( 0 or 1) about 88.4% of the instances in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a12e97b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 500us/step\n",
      "Neural Network accuracy:  0.8843537414965986\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the Neural Network model on the test set\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "nn_predictions = (model.predict(X_test) > 0.5).astype(int)\n",
    "nn_accuracy = accuracy_score(y_test, nn_predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Neural Network accuracy: ', nn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff0503",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034af6b2",
   "metadata": {},
   "source": [
    "Let us now test the finished neural network with fictitious data to see if we get an attrition value of (0 or 1). For this I will create a new dataframe called **new_data** which will have fictitious data that is both featured scaled and has applied dummy variables.\n",
    "\n",
    "The result from running the code below indicates a value of 1 , which means that the employee given the data  traits we entered will face attrition. Note, that running the code below can result in a different answer every time , as the model is 88% accurate with a 12% chance of making errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1de3dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.DataFrame({\n",
    "    'Age': [0.446350],\n",
    "    'DailyRate': [0.742527],\n",
    "    'DistanceFromHome': [-1.010909],\n",
    "    'Education': [-0.891688],\n",
    "    'EmployeeCount': [0.000000],\n",
    "    'EmployeeNumber': [-1.701283],\n",
    "    'EnvironmentSatisfaction': [-0.660531],\n",
    "    'HourlyRate': [1.383138],\n",
    "    'JobInvolvement': [0.379672],\n",
    "    'JobLevel': [-0.057788],\n",
    "    'JobSatisfaction': [1.153254],\n",
    "    'MonthlyIncome': [-0.108350],\n",
    "    'MonthlyRate': [0.726020],\n",
    "    'NumCompaniesWorked': [2.125136],\n",
    "    'PercentSalaryHike': [-1.150554],\n",
    "    'PerformanceRating': [-0.426230],\n",
    "    'RelationshipSatisfaction': [-1.584178],\n",
    "    'StandardHours': [0.000000],\n",
    "    'StockOptionLevel': [-0.932014],\n",
    "    'TotalWorkingYears': [-0.421642],\n",
    "    'TrainingTimesLastYear': [-2.171982],\n",
    "    'WorkLifeBalance': [-2.493820],\n",
    "    'YearsAtCompany': [-0.164613],\n",
    "    'YearsInCurrentRole': [-0.063296],\n",
    "    'YearsSinceLastPromotion': [-0.679146],\n",
    "    'YearsWithCurrManager': [0.245834],\n",
    "    'BusinessTravel_Travel_Frequently': [0.000000],\n",
    "    'BusinessTravel_Travel_Rarely': [1.000000],\n",
    "    'Department_Research & Development': [0.000000],\n",
    "    'Department_Sales': [1.000000],\n",
    "    'EducationField_Life Sciences': [1.000000],\n",
    "    'EducationField_Marketing': [0.000000],\n",
    "    'EducationField_Medical': [0.000000],\n",
    "    'EducationField_Other': [0.000000],\n",
    "    'EducationField_Technical Degree': [0.000000],\n",
    "    'Gender_Male': [0.000000],\n",
    "    'JobRole_Human Resources': [0.000000],\n",
    "    'JobRole_Laboratory Technician': [0.000000],\n",
    "    'JobRole_Manager': [0.000000],\n",
    "    'JobRole_Manufacturing Director': [0.000000],\n",
    "    'JobRole_Research Director': [0.000000],\n",
    "    'JobRole_Research Scientist': [0.000000],\n",
    "    'JobRole_Sales Executive': [1.000000],\n",
    "    'JobRole_Sales Representative': [0.000000],\n",
    "    'MaritalStatus_Married': [0.000000],\n",
    "    'MaritalStatus_Single': [1.000000],\n",
    "    'OverTime_Yes': [1.000000]\n",
    "})\n",
    "\n",
    "# Keep a copy of the original DataFrame\n",
    "X_train_df = pd.DataFrame(X_train, columns=features.columns)\n",
    "\n",
    "# Ensure the new data has the same features as the training set\n",
    "missing_cols = set(X_train_df.columns) - set(new_data.columns)\n",
    "for c in missing_cols:\n",
    "    new_data[c] = 0\n",
    "new_data = new_data[X_train_df.columns]\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(new_data)\n",
    "\n",
    "# The output is a probability, so you might want to convert it to a class label\n",
    "prediction_label = (prediction > 0.5).astype(int)\n",
    "\n",
    "print(prediction_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad37d2",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078ee6a",
   "metadata": {},
   "source": [
    "Using the same data split we used in the neural network (**X_train**,**y_train**,**X_test**, and **y_test**) , lets now create a logistic regression model and compare its accuracy with the neural network.\n",
    "\n",
    "The results from running the code below, convey that the logistic regression model is 89.8% accurate. This means that this model is slightly more accurate than the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fbaf0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy:  0.8979591836734694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "logreg_predictions = logreg.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Logistic Regression model\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Logistic Regression accuracy: ', logreg_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590f71f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9117b9",
   "metadata": {},
   "source": [
    "# Comparing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a98a09",
   "metadata": {},
   "source": [
    "Based on these results, the logistic regression model performed slightly better on the test set than the neural network model.\n",
    "\n",
    "However, keep in mind that this doesn't necessarily mean that logistic regression is a better choice for this particular problem. The performance of a model can depend on many factors, including the architecture of the model (for neural networks), the hyperparameters, and the way the data is preprocessed.\n",
    "\n",
    "It's also important to note that accuracy is just one measure of model performance. Depending on the problem, other metrics like precision, recall, or the area under the ROC curve might be more appropriate.\n",
    "\n",
    "Finally, remember that these results here are based on a single split of the data into a training set(80%) and a test set(20%). To get a more reliable estimate of model performance, we could have also used cross-validation, which involved training and testing the models on different splits of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
